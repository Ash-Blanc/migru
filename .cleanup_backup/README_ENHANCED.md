# Migru - Privacy-First AI Companion for Migraine & Stress Relief

Migru is a **local-first**, personal, and private AI-powered companion designed to support you through migraines and stress with empathy and research-backed relief strategies. It combines ultra-fast local responses with optional web search for complete privacy control.

## ğŸŒŸ Key Features

- **ğŸ”’ Privacy-First Local LLM Integration**: Complete local AI processing with FunctionGemma, Qwen2.5, and other models
- **ğŸ§  Smart Agent Routing**: FunctionGemma-powered router for intelligent agent selection
- **ğŸŒ¿ Empathetic Conversations**: Local models optimized for therapeutic support
- **ğŸ” Optional Web Search**: Privacy-aware search tools only when you want them
- **âš¡ Ultra-Fast Responses**: Local inference eliminates network latency
- **ğŸ¨ Beautiful CLI**: Rich themes and accessibility features
- **ğŸ“Š Real-time Analytics**: Pattern detection and wellness insights

## ğŸ› ï¸ Installation

### Quick Setup with Local LLM Support

```bash
# Clone the repository
git clone <repository-url>
cd migru

# Run the enhanced setup script
./setup_local.sh
```

The setup script will:
- âœ… Check Python 3.12+ and install dependencies
- âœ… Set up llama.cpp server with FunctionGemma
- âœ… Download optimized local models (Qwen2.5:3B, FunctionGemma:7B)
- âœ… Configure privacy-first environment
- âœ… Create startup scripts and test tools

### Manual Setup

```bash
# Install dependencies
uv sync --dev

# Install llama.cpp (recommended for local models)
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
make LLAMA_OPENBLAS=1

# Download FunctionGemma model
mkdir -p models
cd models
wget https://huggingface.co/google/gemma-7b-it/resolve/main/gemma-7b-it.gguf -O function-gemma-7b.gguf

# Configure environment
cp .env.example .env
```

## ğŸš€ Usage

### Start Local LLM Server

```bash
# Start the llama.cpp server
./start_local_server.sh
```

### Run Migru

```bash
# Start with local models
uv run -m app.main_enhanced

# Or with custom user
uv run -m app.main_enhanced --user YourName
```

## ğŸ”’ Privacy Modes

### Local Mode (100% Private)
- **AI Processing**: 100% local, no external APIs
- **Web Search**: Disabled completely
- **Data**: Never leaves your device
- **Recommended**: For sensitive conversations and complete privacy

### Hybrid Mode (Local AI + Optional Search)
- **AI Processing**: Local models for conversations
- **Web Search**: Available when explicitly needed
- **Control**: You choose when to use external services
- **Recommended**: Balance of privacy and functionality

### Flexible Mode (User Choice)
- **AI Processing**: Local or cloud based on preference
- **Web Search**: Always available
- **Control**: Full flexibility per session
- **Recommended**: Power users who want options

## ğŸ§  Local Models

### Recommended Models

| Model | Size | Best For | Privacy Mode |
|--------|-------|-----------|--------------|
| **FunctionGemma 7B** | 7B | Routing, Tool Calling, Research | All |
| **Qwen2.5 3B** | 3B | Empathetic Support, Speed | All |
| **Phi3.5 3.8B** | 3.8B | Balanced Reasoning, Advice | All |
| **Gemma2 2B** | 2B | Lightweight, Fallback | All |

### Model Selection

Migru automatically selects the optimal model based on your conversation type:

- **Emotional Support** â†’ Qwen2.5:3B (warm, empathetic)
- **Research & Tool Calling** â†’ FunctionGemma:7B (reliable, structured)
- **Practical Advice** â†’ Phi3.5:3.8B (balanced reasoning)
- **General Conversation** â†’ Qwen2.5:3B (fast, natural)

## ğŸ® Commands

### Privacy & Model Management

```bash
# Check current privacy settings
/privacy status

# Switch privacy mode
/privacy local      # 100% private
/privacy hybrid     # Local AI + optional search  
/privacy flexible   # User choice

# Manage local models
/local status        # Show current model
/local models        # List available models
/local switch qwen2.5:3b    # Switch models
/local test         # Test connection
```

### Conversation & Search

```bash
# Natural conversation
I'm feeling anxious today
Help me understand my migraines

# Search (when allowed)
/search latest migraine research
/weather New York
/research natural remedies for headaches
```

### Wellness Features

```bash
/patterns             # View your wellness patterns
/insights             # Get personalized insights  
/nudges               # Show wellness suggestions
/theme ocean           # Switch UI theme
```

### System Controls

```bash
/status               # System status
/help                 # Show all commands
/quit                 # Exit
```

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file with your preferences:

```bash
# Local LLM Configuration
LOCAL_LLM_ENABLED=true
LOCAL_LLM_HOST=http://localhost:8080
LOCAL_LLM_MODEL=function-gemma:7b
PRIVACY_MODE=hybrid
ENABLE_SEARCH_IN_LOCAL_MODE=false

# Local Server Settings
LLAMACPP_HOST=http://localhost:8080
OLLAMA_HOST=http://localhost:11434
LOCAL_SERVER_TYPE=llamacpp

# Cloud Fallbacks (optional)
MISTRAL_API_KEY=your_key_here
CEREBRAS_API_KEY=your_key_here
OPENROUTER_API_KEY=your_key_here

# Search & Weather (optional)
FIRECRAWL_API_KEY=your_key_here
OPENWEATHER_API_KEY=your_key_here

# Database
REDIS_URL=redis://localhost:6379
```

## ğŸ—ï¸ Architecture

### Smart Router System

```
User Message â†’ FunctionGemma Router â†’ Task Analysis â†’ Optimal Agent Selection
                                     â†“
                    Emotional Support â†’ Qwen2.5:3B â†’ Companion Agent
                    Research         â†’ FunctionGemma:7B â†’ Researcher Agent  
                    Practical Advice  â†’ Phi3.5:3.8B â†’ Advisor Agent
```

### Privacy-Aware Tools

```
Privacy Mode Controls:
â”œâ”€â”€ Local Mode     â†’ No external APIs, 100% private
â”œâ”€â”€ Hybrid Mode    â†’ Local AI + optional search tools
â””â”€â”€ Flexible Mode  â†’ User choice per interaction

Search Tools:
â”œâ”€â”€ DuckDuckGo     â†’ Fast, private search
â”œâ”€â”€ Firecrawl       â†’ Deep web scraping
â””â”€â”€ OpenWeather     â†’ Weather data (optional)
```

## ğŸ§ª Testing

### Run Tests

```bash
# Run all tests
pytest tests/

# Run local LLM tests
pytest tests/unit/test_local_integration.py -v

# Run with coverage
pytest tests/ --cov=app --cov-report=term-missing

# Test specific components
pytest tests/unit/test_local_integration.py::TestLocalLlamaModel -v
```

### Local Setup Test

```bash
# Test your local LLM setup
python3 test_local_setup.py
```

## ğŸŒ¿ Benefits

### Privacy Benefits
- **Complete Data Control**: All conversations processed locally
- **No Data Leakage**: Optional external calls only with consent
- **Offline Capability**: Works without internet connection
- **User Sovereignty**: Full control over AI model and data

### Performance Benefits  
- **Ultra-Fast Responses**: Local inference eliminates network latency
- **Cost Efficiency**: No API calls for basic conversations
- **Reliability**: Works even when cloud services are down
- **Resource Optimized**: Small models for efficient memory usage

### Flexibility Benefits
- **Gradual Migration**: Start hybrid, move to fully local
- **Model Choice**: Support for multiple local LLM servers
- **Runtime Control**: Switch privacy modes without restarting
- **Optional Features**: Search available when needed, disabled by default

## ğŸ”§ Troubleshooting

### Local Model Issues

```bash
# Check model files
ls -la ~/llama.cpp/models/

# Test connection
curl http://localhost:8080/health

# Check server logs
./start_local_server.sh
```

### Performance Optimization

```bash
# Use smaller models for faster responses
export LOCAL_LLM_MODEL=qwen2.5:3b

# Reduce context for speed
export NUM_HISTORY_RUNS=1

# Disable tools for simplicity
export ENABLE_SEARCH_IN_LOCAL_MODE=false
```

### Common Issues

1. **Model not found**: Ensure models are downloaded correctly
2. **Server won't start**: Check GPU drivers and system resources
3. **Slow responses**: Try smaller models or check system memory
4. **Search disabled**: Switch to hybrid/flexible privacy mode

## ğŸ“š Development

### Project Structure

```
app/
â”œâ”€â”€ models/                # Local LLM integration
â”‚   â””â”€â”€ local_llm.py    # Local model management
â”œâ”€â”€ agents/               # Smart routing system
â”‚   â””â”€â”€ smart_router.py   # FunctionGemma router
â”œâ”€â”€ core.py               # Enhanced Migru core
â”œâ”€â”€ tools/                # Privacy-aware tools
â”‚   â””â”€â”€ privacy_tools.py # Search with privacy controls
â”œâ”€â”€ main_enhanced.py     # Updated CLI interface
â””â”€â”€ config_enhanced.py   # Enhanced configuration
```

### Adding New Local Models

```python
# Add to model_manager.model_configs
"new-model:4b": {
    "description": "New model description",
    "best_for": ["specific_task"],
    "temperature": 0.7,
    "max_tokens": 2048
}
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new features
4. Ensure privacy-first approach
5. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- **FunctionGemma**: Google for tool calling capabilities
- **Qwen2.5**: Alibaba for efficient empathetic models  
- **llama.cpp**: Georgi Gerganov for fast local inference
- **Agno Framework**: For agent orchestration and tools

---

ğŸŒ¸ **Enjoy your private AI companion!** 

For questions, issues, or feature requests, please open an issue on GitHub.